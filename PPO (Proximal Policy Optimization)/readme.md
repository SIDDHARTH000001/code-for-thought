# ğŸ§  Proximal Policy Optimization (PPO)

## ğŸ“Œ Overview
- PPO is a **policy gradient method** for reinforcement learning.
- Developed by **OpenAI (2017)** as a simpler, more stable alternative to TRPO.
- Balances **exploration vs exploitation** while avoiding large destructive policy updates.

---

## âš™ï¸ Key Concepts
- **Policy Gradient**: Directly optimizes the agentâ€™s policy using gradient ascent.
- **Clipped Objective Function**:
  - Prevents updates that change the policy too much.
  - Uses a ratio `r(Î¸)` between new and old policy probabilities.
  - Objective:
    

\[
    L^{CLIP}(\theta) = \mathbb{E} \left[ \min(r(\theta) \hat{A}, \text{clip}(r(\theta), 1-\epsilon, 1+\epsilon)\hat{A}) \right]
    \]


- **Advantage Estimation**:
  - Uses Generalized Advantage Estimation (GAE) for variance reduction.
- **On-policy**:
  - Learns from data generated by the current policy.

---

## ğŸš€ PPO Algorithm Steps
1. Collect trajectories using current policy.
2. Compute rewards-to-go and advantages.
3. Update policy using **clipped surrogate objective**.
4. Repeat with new trajectories.

---

## âœ… Advantages
- Simpler than TRPO (no complex constraints).
- Stable training with fewer tuning requirements.
- Widely used in robotics, games, and large-scale RL tasks.

---

## âš ï¸ Limitations
- Still on-policy â†’ less sample
